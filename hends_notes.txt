My thought process as I go with this project: (my own logs XD)

- I want to download the datsets in code so anyone can just run the project direclty with ease.
I faced issues witht he robot dtatset link not working and retunrning 404 so I downloaded it manully 

- I also found the inside of the datasets is different so I had to load them in different ways.
the breast cancer dataset was the trickest and I had to study the structure and loop over it

- Made this project a repo 

- Preprocessing chapter:
I want to generilize the preprocssing as much as I can. 
    so far I can think of:
    - train_test_split
    - standardize
    - encode_labels


Glass -> Already numeric labels (1–7) but it's ok to encode them I think (not going to)
Energy -> We need to choose either Y1 or Y2 as a single regression target since each model expects one target variable 
Breast Cancer -> Label already converted to 0/1, features are numeric
> For those: I can (use labels as-is) so it's split -> standarize


Pump -> machine_status is a string (NORMAL, etc.) -> Needs encoding	-> use encode_labels()
Robot -> Label is a string (normal, collision, etc.) -> Needs encoding -> use encode_labels()
> For those: I can encode labels -> split -> standarize



Mushroom -> Needs full feature encoding	All features are strings! Definit need for encoding –> one-hot or label encoding
I'm against doing the encoding like ['a', 'b', 'c'] → [0, 1, 2] coz I don't like the idea that 2>0 so I'll go with one-hot and hope that I don't regret anything


------------------------

creating the models chapter:

>>>> logistic_regression
I'm thinking of the normal one of 2 classes and the the other one with multi-classes. 

The accuracy intially:
    - for the breast-cancer -> 92%  | lr=0.01, epochs=1000
    - for the glass -> 39.5%  | lr=0.01, epochs=1000
    - for pump -> ? |  lr=0.01, epochs=1000

Stop. with the pump, the way I wrote the MulticlassLogisticRegression with pure python took ages to train the pumps data. So I had to use numpy 
I feel I'm re inventing the wheel but it feels good to understand things in practice took

Moving on:
    - for the breast-cancer -> 92.98%  | lr=0.01, epochs=1000
    - for the glass -> 58.14%  | lr=0.005, epochs=3000
    - for pump -> 0.9878 |  lr=0.01, epochs=1000
    - for Mashroom -> 97.17%  | lr=0.01, epochs=1000
    - for robot -> 43.01%  | lr=0.01, epochs=1000

built lionear-regression for the energy since it's not classification problem
    - MSE: 11.5071
    - MAE: 2.6759
    - R²: 0.8799

NEXT!!!!

Naive Bayes the smart voting system LOL

- removed standarization so we can deal witht he categories as not numbers

I had to use the labels for the ploting and it mostly didn't do great overall
    - Glass Accuracy: 0.5349
    - Pump Accuracy: 0.9938
    - Breast Cancer Accuracy: 0.7018
    - Mushroom Accuracy: 0.9822
    - Robot Accuracy: 0.2366

As we can see it made great results with pump and mashroom but not so great witht he float heavy featuresso let's try doing the Gaussian Naive Bayes for those

    - Glass Accuracy for gnb: 0.5581
    - Pump Accuracy for gnb: 0.9822
    - Breast Cancer Accuracy for gnb: 0.8947
    - Mushroom Accuracy for gnb: 0.9502
    - Robot Accuracy for gnb: 0.3763

a defet for the robot but better than nothing

(venv) PS C:\Users\100990848\G\uni\ML_code> python -m experiments.run_knn

Glass: 171 train / 43 test
>>>>>>>>  Glass Accuracy for nb: 0.6977


Pump: 176256 train / 44064 test
>>>>>>>>  Pump Accuracy for nb: 0.9999
Confusion Matrix:
     0  1  2
  0: 41238   0   0
  1:  1   0   0
  2:  5   0  2820

Breast Cancer: 455 train / 114 test
>>>>>>>>  Breast Cancer Accuracy for nb: 0.9035
Confusion Matrix:
     0  1
  0: 63   0
  1: 11  40

Mushroom: 6499 train / 1625 test
>>>>>>>>  Mushroom Accuracy for nb: 1.0000
Confusion Matrix:
     0  1
  0: 795   0
  1:  0  830

Robot: 370 train / 93 test
>>>>>>>>  Robot Accuracy for nb: 0.3548
Confusion Matrix:
     0  1  2  3  4  6  7  8  9  10  11  12  13  14  15
  0: 22   0   0   0   0   0   0   2   0   0   0   0   0   0   2
  1:  0   2   0   0   0   0   0   0   0   0   0   3   4   0   3
  2:  0   0   8   0   0   0   0   0   0   0   0   0   0   1   0
  3:  3   0   1   0   0   0   0   0   0   0   0   0   0   0   0
  4:  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  6:  1   0   0   0   0   0   0   0   0   1   0   0   1   0   0
  7:  0   0   0   0   0   0   0   0   1   2   0   1   0   0   0
  8:  6   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  9:  1   0   0   0   0   0   0   0   0   0   0   0   2   0   0
 10:  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
 11:  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0
 12:  1   0   0   0   1   1   0   0   0   2   0   1   0   0   0
 13:  1   8   0   0   0   0   0   0   1   0   0   0   0   0   0
 14:  0   0   4   0   0   0   0   0   0   0   0   0   0   0   0
 15:  0   4   1   0   0   0   0   0   0   0   0   0   0   0   0


 for the tree results:
 (venv) PS C:\Users\100990848\G\uni\ML_code> python -m experiments.run_decision_tree

Glass: 171 train / 43 test
>>>>>>>>  Glass Accuracy: 0.5814
Confusion Matrix:
     0  1  2  3  4  5
  0: 12   4   1   0   0   0
  1:  2   7   0   0   1   0
  2:  2   1   0   0   0   0
  3:  0   1   0   1   0   1
  4:  0   1   0   0   1   0
  5:  0   3   1   0   0   4

Breast Cancer: 455 train / 114 test
>>>>>>>>  Breast Cancer Accuracy: 0.9123
Confusion Matrix:
     0  1
  0: 60   3
  1:  7  44

Mushroom: 6499 train / 1625 test
>>>>>>>>  Mushroom Accuracy: 1.0000
Confusion Matrix:
     0  1
  0: 795   0
  1:  0  830

Robot: 370 train / 93 test
>>>>>>>>  Robot Accuracy: 0.3548
Confusion Matrix:
     0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
  0: 25   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0
  1:  0   3   0   0   0   0   0   0   0   0   0   0   2   4   0   3
  2:  0   0   5   2   0   0   0   0   0   0   0   0   0   0   2   0
  3:  2   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0
  4:  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  5:  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  6:  0   0   0   0   0   0   0   0   0   0   2   0   0   1   0   0
  7:  0   0   0   0   0   0   0   0   0   2   1   0   1   0   0   0
  8:  6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  9:  0   0   0   0   0   1   0   0   0   0   0   0   0   2   0   0
 10:  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
 11:  0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0
 12:  0   1   0   0   1   0   0   0   0   1   3   0   0   0   0   0
 13:  0   8   0   0   0   1   0   1   0   0   0   0   0   0   0   0
 14:  0   0   4   0   0   0   0   0   0   0   0   0   0   0   0   0
 15:  0   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0